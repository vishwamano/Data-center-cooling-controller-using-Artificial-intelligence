{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "AI_cooling_system.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dlg90vOy33W"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Building the Environment\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# BUILDING THE ENVIRONMENT IN A CLASS\n",
        "\n",
        "class Environment(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n",
        "    \n",
        "    def __init__(self, optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 10, initial_rate_data = 60):\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n",
        "        self.initial_month = initial_month\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n",
        "    \n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        \n",
        "          # GETTING THE REWARD\n",
        "        \n",
        "        # Computing the energy spent by the server's cooling system when there is no AI\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        # Scaling the Reward\n",
        "        self.reward = 1e-3 * self.reward\n",
        "        \n",
        "        # GETTING THE NEXT STATE\n",
        "        \n",
        "        # Updating the atmospheric temperature\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server's Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server's Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "        \n",
        "        # GETTING GAME OVER\n",
        "        \n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "        \n",
        "        # UPDATING THE SCORES\n",
        "        \n",
        "        # Updating the Total Energy spent by the AI\n",
        "        self.total_energy_ai += energy_ai\n",
        "        # Updating the Total Energy spent by the server's cooling system when there is no AI\n",
        "        self.total_energy_noai += energy_noai\n",
        "        \n",
        "        # SCALING THE NEXT STATE\n",
        "        \n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        \n",
        "        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER\n",
        "        \n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "    # MAKING A METHOD THAT RESETS THE ENVIRONMENT\n",
        "    \n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT GIVES US AT ANY TIME THE CURRENT STATE, THE LAST REWARD AND WHETHER THE GAME IS OVER\n",
        "    \n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        return current_state, self.reward, self.game_over\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5GwK-ZGz3zC"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Building the Brain without Dropout\n",
        "\n",
        "# Importing the libraries\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# BUILDING THE BRAIN\n",
        "\n",
        "class Brain(object):\n",
        "    \n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\n",
        "        states = Input(shape = (3,))\n",
        "        \n",
        "        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        \n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n",
        "        \n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "        \n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fwNUcNoz72p"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Implementing Deep Q-Learning with Experience Replay\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n",
        "\n",
        "class DQN(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
        "    def __init__(self, max_memory = 100, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]\n",
        "        num_outputs = model.output_shape[-1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            targets[i] = model.predict(current_state)[0]\n",
        "            Q_sa = np.max(model.predict(next_state)[0])\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPlvcnBv3fQH",
        "outputId": "33e60112-e968-4968-bdc1-eb9ba81517c6"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Training the AI without Early Stopping\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "epsilon = .3\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "number_epochs = 100\n",
        "max_memory = 3000\n",
        "batch_size = 512\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
        "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
        "\n",
        "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
        "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = True\n",
        "\n",
        "# TRAINING THE AI\n",
        "env.train = train\n",
        "model = brain.model\n",
        "if (env.train):\n",
        "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
        "    for epoch in range(1, number_epochs):\n",
        "        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
        "        total_reward = 0\n",
        "        loss = 0.\n",
        "        new_month = np.random.randint(0, 12)\n",
        "        env.reset(new_month = new_month)\n",
        "        game_over = False\n",
        "        current_state, _, _ = env.observe()\n",
        "        timestep = 0\n",
        "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
        "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
        "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, number_actions)\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # PLAYING THE NEXT ACTION BY INFERENCE\n",
        "            else:\n",
        "                q_values = model.predict(current_state)\n",
        "                action = np.argmax(q_values[0])\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + int(timestep/(30*24*60)) ) % 12)\n",
        "            total_reward += reward\n",
        "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
        "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
        "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
        "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
        "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            timestep += 1\n",
        "            current_state = next_state\n",
        "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
        "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "        # SAVING THE MODEL\n",
        "        model.save(\"model.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 001/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 002/100\n",
            "Total Energy spent with an AI: 10\n",
            "Total Energy spent with no AI: 18\n",
            "\n",
            "\n",
            "Epoch: 003/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 004/100\n",
            "Total Energy spent with an AI: 6\n",
            "Total Energy spent with no AI: 20\n",
            "\n",
            "\n",
            "Epoch: 005/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 006/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 1\n",
            "\n",
            "\n",
            "Epoch: 007/100\n",
            "Total Energy spent with an AI: 26\n",
            "Total Energy spent with no AI: 34\n",
            "\n",
            "\n",
            "Epoch: 008/100\n",
            "Total Energy spent with an AI: 45\n",
            "Total Energy spent with no AI: 53\n",
            "\n",
            "\n",
            "Epoch: 009/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 20\n",
            "\n",
            "\n",
            "Epoch: 010/100\n",
            "Total Energy spent with an AI: 60\n",
            "Total Energy spent with no AI: 46\n",
            "\n",
            "\n",
            "Epoch: 011/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 4\n",
            "\n",
            "\n",
            "Epoch: 012/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 10\n",
            "\n",
            "\n",
            "Epoch: 013/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 014/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 015/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 11\n",
            "\n",
            "\n",
            "Epoch: 016/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 3\n",
            "\n",
            "\n",
            "Epoch: 017/100\n",
            "Total Energy spent with an AI: 56\n",
            "Total Energy spent with no AI: 59\n",
            "\n",
            "\n",
            "Epoch: 018/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 2\n",
            "\n",
            "\n",
            "Epoch: 019/100\n",
            "Total Energy spent with an AI: 22\n",
            "Total Energy spent with no AI: 46\n",
            "\n",
            "\n",
            "Epoch: 020/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 12\n",
            "\n",
            "\n",
            "Epoch: 021/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 022/100\n",
            "Total Energy spent with an AI: 30\n",
            "Total Energy spent with no AI: 32\n",
            "\n",
            "\n",
            "Epoch: 023/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 5\n",
            "\n",
            "\n",
            "Epoch: 024/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 025/100\n",
            "Total Energy spent with an AI: 15\n",
            "Total Energy spent with no AI: 19\n",
            "\n",
            "\n",
            "Epoch: 026/100\n",
            "Total Energy spent with an AI: 12\n",
            "Total Energy spent with no AI: 11\n",
            "\n",
            "\n",
            "Epoch: 027/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 028/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 4\n",
            "\n",
            "\n",
            "Epoch: 029/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 030/100\n",
            "Total Energy spent with an AI: 36\n",
            "Total Energy spent with no AI: 67\n",
            "\n",
            "\n",
            "Epoch: 031/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 032/100\n",
            "Total Energy spent with an AI: 15\n",
            "Total Energy spent with no AI: 21\n",
            "\n",
            "\n",
            "Epoch: 033/100\n",
            "Total Energy spent with an AI: 16\n",
            "Total Energy spent with no AI: 29\n",
            "\n",
            "\n",
            "Epoch: 034/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 24\n",
            "\n",
            "\n",
            "Epoch: 035/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 12\n",
            "\n",
            "\n",
            "Epoch: 036/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 3\n",
            "\n",
            "\n",
            "Epoch: 037/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 038/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 039/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 38\n",
            "\n",
            "\n",
            "Epoch: 040/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 041/100\n",
            "Total Energy spent with an AI: 24\n",
            "Total Energy spent with no AI: 20\n",
            "\n",
            "\n",
            "Epoch: 042/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 043/100\n",
            "Total Energy spent with an AI: 16\n",
            "Total Energy spent with no AI: 30\n",
            "\n",
            "\n",
            "Epoch: 044/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 045/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 046/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 047/100\n",
            "Total Energy spent with an AI: 24\n",
            "Total Energy spent with no AI: 43\n",
            "\n",
            "\n",
            "Epoch: 048/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 049/100\n",
            "Total Energy spent with an AI: 33\n",
            "Total Energy spent with no AI: 51\n",
            "\n",
            "\n",
            "Epoch: 050/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 2\n",
            "\n",
            "\n",
            "Epoch: 051/100\n",
            "Total Energy spent with an AI: 28\n",
            "Total Energy spent with no AI: 51\n",
            "\n",
            "\n",
            "Epoch: 052/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 053/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 30\n",
            "\n",
            "\n",
            "Epoch: 054/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 7\n",
            "\n",
            "\n",
            "Epoch: 055/100\n",
            "Total Energy spent with an AI: 24\n",
            "Total Energy spent with no AI: 49\n",
            "\n",
            "\n",
            "Epoch: 056/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 057/100\n",
            "Total Energy spent with an AI: 15\n",
            "Total Energy spent with no AI: 2\n",
            "\n",
            "\n",
            "Epoch: 058/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 7\n",
            "\n",
            "\n",
            "Epoch: 059/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 060/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 061/100\n",
            "Total Energy spent with an AI: 45\n",
            "Total Energy spent with no AI: 104\n",
            "\n",
            "\n",
            "Epoch: 062/100\n",
            "Total Energy spent with an AI: 22\n",
            "Total Energy spent with no AI: 42\n",
            "\n",
            "\n",
            "Epoch: 063/100\n",
            "Total Energy spent with an AI: 26\n",
            "Total Energy spent with no AI: 62\n",
            "\n",
            "\n",
            "Epoch: 064/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 4\n",
            "\n",
            "\n",
            "Epoch: 065/100\n",
            "Total Energy spent with an AI: 21\n",
            "Total Energy spent with no AI: 18\n",
            "\n",
            "\n",
            "Epoch: 066/100\n",
            "Total Energy spent with an AI: 10\n",
            "Total Energy spent with no AI: 9\n",
            "\n",
            "\n",
            "Epoch: 067/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 11\n",
            "\n",
            "\n",
            "Epoch: 068/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 3\n",
            "\n",
            "\n",
            "Epoch: 069/100\n",
            "Total Energy spent with an AI: 39\n",
            "Total Energy spent with no AI: 93\n",
            "\n",
            "\n",
            "Epoch: 070/100\n",
            "Total Energy spent with an AI: 15\n",
            "Total Energy spent with no AI: 26\n",
            "\n",
            "\n",
            "Epoch: 071/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 072/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 073/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 2\n",
            "\n",
            "\n",
            "Epoch: 074/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 1\n",
            "\n",
            "\n",
            "Epoch: 075/100\n",
            "Total Energy spent with an AI: 16\n",
            "Total Energy spent with no AI: 27\n",
            "\n",
            "\n",
            "Epoch: 076/100\n",
            "Total Energy spent with an AI: 15\n",
            "Total Energy spent with no AI: 16\n",
            "\n",
            "\n",
            "Epoch: 077/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 078/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 079/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 3\n",
            "\n",
            "\n",
            "Epoch: 080/100\n",
            "Total Energy spent with an AI: 12\n",
            "Total Energy spent with no AI: 20\n",
            "\n",
            "\n",
            "Epoch: 081/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 082/100\n",
            "Total Energy spent with an AI: 26\n",
            "Total Energy spent with no AI: 39\n",
            "\n",
            "\n",
            "Epoch: 083/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 084/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 28\n",
            "\n",
            "\n",
            "Epoch: 085/100\n",
            "Total Energy spent with an AI: 6\n",
            "Total Energy spent with no AI: 22\n",
            "\n",
            "\n",
            "Epoch: 086/100\n",
            "Total Energy spent with an AI: 6\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 087/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVI5Cc_e4auz"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Testing the AI\n",
        "\n",
        "# Installing Keras\n",
        "# conda install -c conda-forge keras\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# LOADING A PRE-TRAINED BRAIN\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = False\n",
        "\n",
        "# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\n",
        "env.train = train\n",
        "current_state, _, _ = env.observe()\n",
        "for timestep in range(0,24 * 60):\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values[0])\n",
        "    if (action - direction_boundary < 0):\n",
        "        direction = -1\n",
        "    else:\n",
        "        direction = 1\n",
        "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
        "    current_state = next_state\n",
        "\n",
        "# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "print(\"\\n\")\n",
        "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "print(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}